{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dummy():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dummy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating datasets\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.path.append('../..')\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import platform\n",
    "import logging\n",
    "from collections import OrderedDict\n",
    "from models.ConvLSTM import EncoderForecaster, Encoder, Forecaster, ConvLSTMCell\n",
    "from utils.io import save_network, save, create_results_folder, save_datasets_to_file\n",
    "from utils.arg_extract import get_args\n",
    "from utils.ExperimentBuilder import ExperimentBuilder\n",
    "from utils.WaveDataset import create_datasets, transformVar, create_dataloaders\n",
    "plt.ioff()\n",
    "logging.basicConfig(format='%(message)s', level=logging.INFO)\n",
    "\n",
    "# args, device = get_args()  # get arguments from command line\n",
    "args.batch_size=6\n",
    "args.experiment_name='ttt'\n",
    "device = 'cpu'\n",
    "# Data\n",
    "if 'Darwin' in platform.system():\n",
    "    base_folder = '/Users/stathis/Code/thesis/wave_propagation/'\n",
    "    data_dir = base_folder\n",
    "else:\n",
    "    base_folder = '/home/s1680171/wave_propagation/'\n",
    "    data_dir = '/disk/scratch/s1680171/wave_propagation/'\n",
    "\n",
    "dirs = create_results_folder(base_folder=base_folder, experiment_name=experiment_name)\n",
    "\n",
    "logging.info('Creating datasets')\n",
    "train_dataset, val_dataset, test_dataset = create_datasets(os.path.join(data_dir, \"Video_Data/\"), \n",
    "                                                           transformVar, test_fraction=0.15,\n",
    "                                                           validation_fraction=0.15)\n",
    "# filename_data = os.path.join(dirs['pickles'], \"all_data.pickle\")\n",
    "# save_datasets_to_file(train_dataset, val_dataset, test_dataset, filename_data)\n",
    "\n",
    "train_dataloader, val_dataloader, test_dataloader = create_dataloaders(train_dataset, val_dataset, test_dataset, \n",
    "                                                                       batch_size=batch_size, num_workers=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for batch_num, batch_images in enumerate(train_dataloader):\n",
    "#     print(batch_num, batch_images.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100, 128, 128])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_images.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.num_input_frames = 5\n",
    "args.num_output_frames = 20\n",
    "args.learning_rate = 10e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define encoder #\n",
    "encoder_architecture = [\n",
    "    [   # in_channels, out_channels, kernel_size, stride, padding\n",
    "        OrderedDict({'conv1_leaky_1': [1, 8, 3, 2, 1]}),\n",
    "        OrderedDict({'conv2_leaky_1': [64, 192, 3, 2, 1]}),\n",
    "        OrderedDict({'conv3_leaky_1': [192, 192, 3, 2, 1]}),\n",
    "    ],\n",
    "\n",
    "    [\n",
    "        ConvLSTMCell(input_channel=8, num_filter=64, b_h_w=(args.batch_size, 64, 64),\n",
    "                     kernel_size=3, stride=1, padding=1, device=device, seq_len=args.num_input_frames),\n",
    "        ConvLSTMCell(input_channel=192, num_filter=192, b_h_w=(args.batch_size, 32, 32),\n",
    "                     kernel_size=3, stride=1, padding=1, device=device, seq_len=args.num_input_frames),\n",
    "        ConvLSTMCell(input_channel=192, num_filter=192, b_h_w=(args.batch_size, 16, 16),\n",
    "                     kernel_size=3, stride=1, padding=1, device=device, seq_len=args.num_input_frames),\n",
    "    ]\n",
    "]\n",
    "forecaster_architecture = [\n",
    "    [\n",
    "        OrderedDict({'deconv1_leaky_1': [192, 192, 4, 2, 1]}),\n",
    "        OrderedDict({'deconv2_leaky_1': [192, 64, 4, 2, 1]}),\n",
    "        OrderedDict({\n",
    "            'deconv3_leaky_1': [64, 8, 4, 2, 1],\n",
    "            'conv3_leaky_2': [8, 8, 3, 1, 1],\n",
    "            'conv3_3': [8, 1, 1, 1, 0]\n",
    "        }),\n",
    "    ],\n",
    "\n",
    "    [\n",
    "        ConvLSTMCell(input_channel=192, num_filter=192, b_h_w=(args.batch_size, 16, 16),\n",
    "                     kernel_size=3, stride=1, padding=1, device=device, seq_len=args.num_output_frames),\n",
    "        ConvLSTMCell(input_channel=192, num_filter=192, b_h_w=(args.batch_size, 32, 32),\n",
    "                     kernel_size=3, stride=1, padding=1, device=device, seq_len=args.num_output_frames),\n",
    "        ConvLSTMCell(input_channel=64, num_filter=64, b_h_w=(args.batch_size, 64, 64),\n",
    "                     kernel_size=3, stride=1, padding=1, device=device, seq_len=args.num_output_frames),\n",
    "    ]\n",
    "]\n",
    "\n",
    "encoder = Encoder(encoder_architecture[0], encoder_architecture[1]).to(device)\n",
    "forecaster = Forecaster(forecaster_architecture[0], forecaster_architecture[1], args.num_output_frames).to(device)\n",
    "model = EncoderForecaster(encoder, forecaster)\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters(), amsgrad=False, lr=args.learning_rate, weight_decay=args.weight_decay_coefficient)\n",
    "# lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_ok = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_ok.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 6, 1, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "out = model(batch_ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 20, 128, 128])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.helper_functions import convert_BSHW_to_SBCHW, convert_SBCHW_to_BSHW\n",
    "cout = convert_SBCHW_to_BSHW(out)\n",
    "cout.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1 = model(batch_ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "out2 = model(batch_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 20, 128, 128])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 6, 1, 128, 128])\n",
      "torch.Size([20, 6, 128, 128])\n",
      "torch.Size([6, 20, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "x = out1.clone()\n",
    "print(x.size())\n",
    "x = x.squeeze(2)\n",
    "# x = x.squeeze()\n",
    "print(x.size())\n",
    "x = x.permute(1,0,2,3)\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original  torch.Size([20, 1, 1, 128, 128])\n",
      "squeeze  torch.Size([20, 1, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "x = out2.clone()\n",
    "print('original ', x.size())\n",
    "x = x.squeeze(1)\n",
    "print('squeeze ', x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original  torch.Size([20, 1, 1, 128, 128])\n",
      "squeeze  torch.Size([20, 128, 128])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "number of dims don't match in permute",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-83d224ec84e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'squeeze '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'permute '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: number of dims don't match in permute"
     ]
    }
   ],
   "source": [
    "x = x.permute(1,0,2,3,)\n",
    "print('permute ', x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (stage1): Sequential(\n",
       "    (conv1_leaky_1): Conv2d(1, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (leaky_conv1_leaky_1): LeakyReLU(negative_slope=0.2, inplace)\n",
       "  )\n",
       "  (rnn1): ConvLSTMCell(\n",
       "    (_conv): Conv2d(72, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (stage2): Sequential(\n",
       "    (conv2_leaky_1): Conv2d(64, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (leaky_conv2_leaky_1): LeakyReLU(negative_slope=0.2, inplace)\n",
       "  )\n",
       "  (rnn2): ConvLSTMCell(\n",
       "    (_conv): Conv2d(384, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (stage3): Sequential(\n",
       "    (conv3_leaky_1): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (leaky_conv3_leaky_1): LeakyReLU(negative_slope=0.2, inplace)\n",
       "  )\n",
       "  (rnn3): ConvLSTMCell(\n",
       "    (_conv): Conv2d(384, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('conv1_leaky_1', Conv2d(1, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))), ('leaky_conv1_leaky_1', LeakyReLU(negative_slope=0.2, inplace))]\n",
      "[('conv2_leaky_1', Conv2d(64, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))), ('leaky_conv2_leaky_1', LeakyReLU(negative_slope=0.2, inplace))]\n",
      "[('conv3_leaky_1', Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))), ('leaky_conv3_leaky_1', LeakyReLU(negative_slope=0.2, inplace))]\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(encoder_architecture[0], encoder_architecture[1]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = convert_BSHW_to_SBCHW(batch_images)\n",
    "state = encoder(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100, 128, 128])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_images.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1, 1, 128, 128])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 64, 64])\n",
      "torch.Size([1, 64, 64, 64])\n",
      "torch.Size([1, 192, 32, 32])\n",
      "torch.Size([1, 192, 32, 32])\n",
      "torch.Size([1, 192, 16, 16])\n",
      "torch.Size([1, 192, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "print(state[0][0].size())\n",
    "print(state[0][1].size())\n",
    "print(state[1][0].size())\n",
    "print(state[1][1].size())\n",
    "print(state[2][0].size())\n",
    "print(state[2][1].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=1\n",
    "current_epoch_losses = {\"train_loss\": [], \"val_loss\": []}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "current_epoch_losses[\"train_loss\"].append(loss)\n",
    "current_epoch_losses[\"val_loss\"].append(loss)  # add current iter loss to val loss list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_losses = {\"train_loss\": [], \"val_loss\": [], \"curr_epoch\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in current_epoch_losses.items():\n",
    "    total_losses[key].append(np.mean(value))  #\n",
    "total_losses['curr_epoch'].append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_loss': [1.0], 'val_loss': [1.0], 'curr_epoch': [1]}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Train loss: 1.0000 | Validation loss: 1.0000'"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Train loss: {:.4f} | Validation loss: {:.4f}\".format(total_losses['train_loss'][-1], total_losses['val_loss'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
