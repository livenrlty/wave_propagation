{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import logging\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import platform\n",
    "import time\n",
    "from models.AR_LSTM import AR_LSTM, train_epoch, validate, test\n",
    "from utils.Analyser import Analyser\n",
    "from utils.io import save_network, save, create_results_folder, save_datasets_to_file\n",
    "from utils.WaveDataset import create_datasets, transformVar, create_dataloaders\n",
    "from utils.arg_extract import get_args\n",
    "from utils.Scorekeeper import Scorekeeper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize = transforms.Normalize(mean=[0.5047], std=[0.1176])\n",
    "\n",
    "transformVar = {\"Test\": transforms.Compose([\n",
    "    transforms.Resize(128),    #Already 184 x 184\n",
    "    transforms.CenterCrop(128),\n",
    "    transforms.ToTensor(),\n",
    "#     normalize\n",
    "]),\n",
    "    \"Train\": transforms.Compose([\n",
    "    transforms.Resize(128),  # Already 184 x 184\n",
    "    transforms.CenterCrop(128),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "#     normalize\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-63471188bbfe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# save_datasets_to_file(train_dataset, val_dataset, test_dataset, filename_data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_dataloaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'batch_size' is not defined"
     ]
    }
   ],
   "source": [
    "if 'Darwin' in platform.system():\n",
    "    base_folder = '/Users/stathis/Code/thesis/wave_propagation/'\n",
    "    data_dir = base_folder\n",
    "else:\n",
    "    base_folder = '/home/s1680171/wave_propagation/'\n",
    "    data_dir = '/disk/scratch/s1680171/wave_propagation/'\n",
    "\n",
    "\n",
    "# dirs = create_results_folder(base_folder=base_folder, experiment_name=.experiment_name)\n",
    "\n",
    "logging.info('Creating datasets')\n",
    "train_dataset, val_dataset, test_dataset = create_datasets(os.path.join(data_dir, \"Video_Data/\"), transformVar, test_fraction=0.15, validation_fraction=0.15)\n",
    "# filename_data = os.path.join(dirs['results'], \"all_data.pickle\")\n",
    "# save_datasets_to_file(train_dataset, val_dataset, test_dataset, filename_data)\n",
    "\n",
    "train_dataloader, val_dataloader, test_dataloader = create_dataloaders(train_dataset, val_dataset, test_dataset, batch_size, num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-8b74baf02efc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "bb = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb['image'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mean=0.\n",
    "std=0.\n",
    "nb_samples=0\n",
    "\n",
    "for bb in train_dataloader:\n",
    "    data=bb['image']\n",
    "    batch_samples = data.size(0)\n",
    "    images_per_video = data.size(1)\n",
    "    data = data.view(batch_samples, images_per_video, -1)\n",
    "    mean += data.mean(2).sum()\n",
    "    std += data.std(2).sum()\n",
    "\n",
    "    nb_samples += batch_samples*images_per_video\n",
    " \n",
    "mean /= nb_samples\n",
    "std /= nb_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5047)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1176)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "320900"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "321600"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "16*100*len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_channels=3\n",
    "num_workers=12\n",
    "\n",
    "test_dataset, val_dataset, train_dataset = create_datasets(\n",
    "     data_dir+\"Video_Data/\", transformVar, test_fraction=0.01, validation_fraction=0.01, check_bad_data=False, channels=num_channels)\n",
    "all_data = {\"Training data\": train_dataset, \"Validation data\": val_dataset, \"Testing data\": test_dataset}\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=num_workers)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=True, num_workers=num_workers)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=True, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean=0.\n",
    "std=0.\n",
    "nb_samples=0\n",
    "\n",
    "for bb in train_dataloader:\n",
    "    data=bb['image']\n",
    "    batch_samples = data.size(0)\n",
    "    images_per_video = data.size(1)\n",
    "    data = data.view(batch_samples, int(images_per_video/3), 3, -1)\n",
    "    mean += data.mean(3).sum(0).sum(0)\n",
    "    std += data.std(3).sum(0).sum(0)\n",
    "\n",
    "    nb_samples += batch_samples*images_per_video\n",
    " \n",
    "mean /= nb_samples\n",
    "std /= nb_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1683, 0.1683, 0.1683])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0392, 0.0392, 0.0392])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "962700"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=bb['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([810.1906, 810.1906, 810.1906])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.view(16,100,3,-1).mean(3).sum(0).sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
