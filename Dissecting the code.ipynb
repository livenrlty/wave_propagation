{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf Results/7_kernel_3LSTM_debug*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating new datasets\n",
      "Creating analyser\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import logging\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import platform\n",
    "import time\n",
    "from utils.Network import Network\n",
    "from utils.Analyser import Analyser\n",
    "from utils.io import save_network, load_network, save, load, make_folder_results\n",
    "from utils.WaveDataset import create_datasets\n",
    "from utils.training import train_epoch, validate, test\n",
    "\n",
    "logging.basicConfig(format='%(message)s',level=logging.INFO)\n",
    "channels=1\n",
    "num_workers=4\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "transformVar = {\"Test\": transforms.Compose([\n",
    "    transforms.Resize(128),    #Already 184 x 184\n",
    "    transforms.CenterCrop(128),\n",
    "    transforms.ToTensor(),\n",
    "]),\n",
    "    \"Train\": transforms.Compose([\n",
    "    transforms.Resize(128),  # Already 184 x 184\n",
    "    transforms.CenterCrop(128),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    ])\n",
    "}\n",
    "\n",
    "nr_net = 0 \n",
    "\n",
    "version = nr_net + 10\n",
    "num_input_frames = 5\n",
    "num_output_frames = 20\n",
    "reinsert_frequency = 10\n",
    "network_type = \"7_kernel_3LSTM_debug\"\n",
    "\n",
    "if 'Darwin' in platform.system():\n",
    "    data_dir = './'\n",
    "else:\n",
    "    data_dir = '/disk/scratch/s1680171/wave_propagation/'\n",
    "\n",
    "if not os.path.isdir(\"./Results\"):\n",
    "    os.mkdir(\"./Results\")\n",
    "results_dir = \"./Results/\" + network_type + \"_v%03d/\" % version\n",
    "\n",
    "if not os.path.isdir(results_dir):\n",
    "    make_folder_results(results_dir)\n",
    "\n",
    "# Data\n",
    "filename_data = results_dir + \"all_data.pickle\"\n",
    "if os.path.isfile(filename_data):\n",
    "    logging.info('Loading datasets')\n",
    "    all_data = load(filename_data)\n",
    "    train_dataset = all_data[\"Training data\"]\n",
    "    val_dataset = all_data[\"Validation data\"]\n",
    "    test_dataset = all_data[\"Testing data\"]\n",
    "else:\n",
    "    logging.info('Creating new datasets')\n",
    "    test_dataset, val_dataset, train_dataset = create_datasets(\n",
    "         data_dir+\"Video_Data/\", transformVar, test_fraction=0.15, validation_fraction=0.15, check_bad_data=False, channels=channels)\n",
    "    all_data = {\"Training data\": train_dataset, \"Validation data\": val_dataset, \"Testing data\": test_dataset}\n",
    "    save(all_data, filename_data)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=num_workers)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=True, num_workers=num_workers)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "\n",
    "# analyser\n",
    "filename_analyser = results_dir + \"analyser.pickle\" \n",
    "if os.path.isfile(filename_analyser):\n",
    "    logging.info('Loading analyser')\n",
    "    analyser = load(filename_analyser)\n",
    "else:\n",
    "    logging.info('Creating analyser')\n",
    "    analyser = Analyser(results_dir)\n",
    "\n",
    "# Model\n",
    "filename_model = results_dir + \"model.pt\"\n",
    "if os.path.isfile(filename_model):\n",
    "    model = Network(device, channels)\n",
    "    model = load_network(model, device, filename_model)\n",
    "else:\n",
    "    model = Network(device, channels)\n",
    "\n",
    "# Learning Rate scheduler w. optimizer\n",
    "# Optimizer\n",
    "optimizer_algorithm = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "# Add learning rate schedulers\n",
    "# Decay LR by a factor of gamma every step_size epochs\n",
    "scheduler_type = 'plateau'\n",
    "if scheduler_type == 'step':\n",
    "    gamma = 0.5\n",
    "    step_size = 40\n",
    "    lr_scheduler = optim.lr_scheduler.StepLR(optimizer_algorithm, step_size=step_size, gamma=gamma)\n",
    "elif scheduler_type == 'plateau':\n",
    "    # Reduce learning rate when a metric has stopped improving\n",
    "    lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer_algorithm, mode='min', factor=0.1, patience=7)\n",
    "\n",
    "filename_metadata = results_dir + \"metadata.pickle\" \n",
    "meta_data_dict = {  \"optimizer\": optimizer_algorithm.state_dict(),\n",
    "                    \"scheduler_type\": scheduler_type, \n",
    "                    \"scheduler\": lr_scheduler.state_dict()}\n",
    "save(meta_data_dict, filename_metadata)\n",
    "\n",
    "model = model.to(device) \n",
    "\n",
    "# analyser = []\n",
    "# model =[]\n",
    "# lr_scheduler = []\n",
    "# scheduler_dict = []\n",
    "\n",
    "# analyser.plot_loss()\n",
    "# analyser.plot_loss_batchwise()\n",
    "# analyser.plot_validation_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment 10\n",
      "Start training\n",
      "Epoch 0\n",
      "Training: Ready to load batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch loss  1.2911416292190552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [1/31 (3%)]\tLoss: 1.291142\tTime 12.50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val batch loss  0.061801813542842865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.061802\tTime: 13.130\n",
      "Epoch 1\n",
      "Training: Ready to load batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch loss  1.0054479837417603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [1/31 (3%)]\tLoss: 1.005448\tTime 14.09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val batch loss  0.06518342345952988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.065183\tTime: 12.912\n",
      "Epoch 2\n",
      "Training: Ready to load batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch loss  0.8339086174964905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [1/31 (3%)]\tLoss: 0.833909\tTime 14.38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val batch loss  0.13548536598682404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.135485\tTime: 13.365\n",
      "Epoch 3\n",
      "Training: Ready to load batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch loss  0.7168624997138977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [1/31 (3%)]\tLoss: 0.716862\tTime 14.86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val batch loss  0.2392827421426773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.239283\tTime: 13.350\n",
      "Epoch 4\n",
      "Training: Ready to load batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch loss  0.6481192708015442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [1/31 (3%)]\tLoss: 0.648119\tTime 15.71\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val batch loss  0.2560531795024872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.256053\tTime: 17.116\n",
      "Epoch 5\n",
      "Training: Ready to load batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch loss  0.5834929943084717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [1/31 (3%)]\tLoss: 0.583493\tTime 16.30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val batch loss  0.2318456768989563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.231846\tTime: 13.732\n",
      "Epoch 6\n",
      "Training: Ready to load batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch loss  0.5415123105049133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [1/31 (3%)]\tLoss: 0.541512\tTime 14.99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val batch loss  0.19040828943252563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.190408\tTime: 13.802\n",
      "Epoch 7\n",
      "Training: Ready to load batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch loss  0.506492018699646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [1/31 (3%)]\tLoss: 0.506492\tTime 16.02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val batch loss  0.17319762706756592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.173198\tTime: 13.155\n",
      "Epoch 8\n",
      "Training: Ready to load batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch loss  0.47281697392463684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8 [1/31 (3%)]\tLoss: 0.472817\tTime 16.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val batch loss  0.1558084487915039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.155808\tTime: 13.768\n",
      "Epoch 9\n",
      "Training: Ready to load batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch loss  0.4484100341796875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [1/31 (3%)]\tLoss: 0.448410\tTime 16.71\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val batch loss  0.14054754376411438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.140548\tTime: 13.815\n",
      "Epoch 10\n",
      "Training: Ready to load batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch loss  0.44418609142303467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 10 [1/31 (3%)]\tLoss: 0.444186\tTime 17.58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val batch loss  0.14024895429611206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.140249\tTime: 13.785\n",
      "Epoch 11\n",
      "Training: Ready to load batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch loss  0.4251880943775177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 11 [1/31 (3%)]\tLoss: 0.425188\tTime 17.58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val batch loss  0.12664102017879486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.126641\tTime: 13.890\n",
      "Epoch 12\n",
      "Training: Ready to load batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch loss  0.39920005202293396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 12 [1/31 (3%)]\tLoss: 0.399200\tTime 18.69\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val batch loss  0.11584629118442535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.115846\tTime: 14.750\n",
      "Epoch 13\n",
      "Training: Ready to load batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch loss  0.4123101234436035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/stathis/anaconda3/envs/thesis/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Users/stathis/anaconda3/envs/thesis/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Users/stathis/anaconda3/envs/thesis/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Users/stathis/anaconda3/envs/thesis/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/Users/stathis/anaconda3/envs/thesis/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Users/stathis/anaconda3/envs/thesis/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Users/stathis/anaconda3/envs/thesis/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Users/stathis/anaconda3/envs/thesis/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/Users/stathis/anaconda3/envs/thesis/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Users/stathis/anaconda3/envs/thesis/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Users/stathis/anaconda3/envs/thesis/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Users/stathis/anaconda3/envs/thesis/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "  File \"/Users/stathis/anaconda3/envs/thesis/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Users/stathis/anaconda3/envs/thesis/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Users/stathis/anaconda3/envs/thesis/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Users/stathis/anaconda3/envs/thesis/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "\n",
      "Unfortunately, your original traceback can not be constructed.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/stathis/anaconda3/envs/thesis/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3291, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-3-22e1fc2f58d9>\", line 9, in <module>\n",
      "    num_output_frames,reinsert_frequency, channels, device, analyser, plot=False)\n",
      "  File \"/Users/stathis/Code/thesis/wave_propagation/utils/training.py\", line 91, in train_epoch\n",
      "    lr_scheduler.optimizer.step()\n",
      "  File \"/Users/stathis/anaconda3/envs/thesis/lib/python3.6/site-packages/torch/optim/adam.py\", line 94, in step\n",
      "    exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/stathis/anaconda3/envs/thesis/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2033, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/stathis/anaconda3/envs/thesis/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/Users/stathis/anaconda3/envs/thesis/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/stathis/anaconda3/envs/thesis/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 347, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/Users/stathis/anaconda3/envs/thesis/lib/python3.6/inspect.py\", line 1490, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/Users/stathis/anaconda3/envs/thesis/lib/python3.6/inspect.py\", line 1448, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/Users/stathis/anaconda3/envs/thesis/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/Users/stathis/anaconda3/envs/thesis/lib/python3.6/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/Users/stathis/anaconda3/envs/thesis/lib/python3.6/posixpath.py\", line 395, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/Users/stathis/anaconda3/envs/thesis/lib/python3.6/posixpath.py\", line 429, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/Users/stathis/anaconda3/envs/thesis/lib/python3.6/posixpath.py\", line 171, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "logging.info('Experiment %d' % version)\n",
    "logging.info('Start training')\n",
    "epochs=50\n",
    "for epoch in range(epochs):\n",
    "    epoch_start = time.time()\n",
    "\n",
    "    logging.info('Epoch %d' % epoch)\n",
    "    train_loss = train_epoch(model, lr_scheduler, epoch, val_dataloader, num_input_frames, \n",
    "                            num_output_frames,reinsert_frequency, channels, device, analyser, plot=False)\n",
    "    analyser.save_epoch_loss(train_loss, 1)\n",
    "    validation_loss = validate(model, train_dataloader, num_input_frames, num_output_frames, reinsert_frequency,\n",
    "                               channels, device, plot=False)\n",
    "#     analyser.save_validation_loss(validation_loss, 1)\n",
    "#     \"\"\"\n",
    "#     Here we can access analyser.validation_loss to make decisions\n",
    "#     \"\"\"\n",
    "#     # Learning rate scheduler\n",
    "#     # perform scheduler step if independent from validation loss\n",
    "#     if scheduler_type == 'step':\n",
    "#         lr_scheduler.step()\n",
    "#     # perform scheduler step if dependent on validation loss\n",
    "#     if scheduler_type == 'plateau':\n",
    "#         validation_loss = analyser.validation_loss[-1]\n",
    "#         lr_scheduler.step(validation_loss)\n",
    "#     save_network(model, filename_model)\n",
    "#     save(analyser, filename_analyser)\n",
    "\n",
    "#     epoch_time = time.time() - epoch_start \n",
    "#     logging.info('Epoch time: %.1f' % epoch_time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
