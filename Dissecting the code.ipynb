{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import logging\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from os import listdir\n",
    "import random\n",
    "import copy\n",
    "from torch.utils.data import DataLoader\n",
    "from skimage import measure #supports video also\n",
    "import pickle\n",
    "import scipy.ndimage as ndimage\n",
    "from scipy.spatial import distance\n",
    "import time\n",
    "import platform\n",
    "\n",
    "from utils.Network import Network\n",
    "from utils.Analyser import Analyser\n",
    "from utils.io import save_network, load_network, save, load, figure_save, make_folder_results, imshow\n",
    "from utils.format import hex_str2bool\n",
    "from utils.WaveDataset import Create_Datasets\n",
    "\n",
    "logging.basicConfig(format='%(message)s',level=logging.INFO)\n",
    "\n",
    "channels=1\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "transformVar = {\"Test\": transforms.Compose([\n",
    "    transforms.Resize(128),    #Already 184 x 184\n",
    "    transforms.CenterCrop(128),\n",
    "    transforms.ToTensor(),\n",
    "#     normalize\n",
    "]),\n",
    "    \"Train\": transforms.Compose([\n",
    "    transforms.Resize(128),  # Already 184 x 184\n",
    "    transforms.CenterCrop(128),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "#     normalize\n",
    "    ])\n",
    "}\n",
    "\n",
    "import time\n",
    "def train_epoch(model, epoch, train_dataloader, val_dataloader, channels, device, plot=False,):\n",
    "    \"\"\"\n",
    "    Training of the network\n",
    "    :param train: Training data\n",
    "    :param val_dataloader: Validation data\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    def initial_input(channels, training):\n",
    "        Data = ImageSeries[:, (t0 + n) * channels:(t0 + n + input_frames) * channels, :, :].to(device)\n",
    "        output = model(Data, training=training)\n",
    "        target = ImageSeries[:, (t0 + n + input_frames) * channels:(t0 + n + input_frames + 1) * channels, :, :].to(device)\n",
    "        return output, target\n",
    "\n",
    "    def new_input(output, target, channels, training):\n",
    "        output = torch.cat((output, model(\n",
    "            output[:, -input_frames * channels:, :, :].clone(), mode=\"new_input\", training=training)\n",
    "                            ), dim=1)\n",
    "        target = torch.cat(\n",
    "            (target, ImageSeries[:, (t0 + n + input_frames) * channels:(t0 + n + input_frames + 1) * channels, :, :].to(device)), dim=1\n",
    "        )\n",
    "        return output, target\n",
    "\n",
    "    def consequent_propagation(output, target, channels, training):\n",
    "        output = torch.cat((output, model(torch.Tensor([0]), mode=\"internal\", training=training)), dim=1)\n",
    "        target = torch.cat(\n",
    "            (target, ImageSeries[:, (t0 + n + input_frames) * channels:(t0 + n + input_frames + 1) * channels, :, :].to(device)), dim=1\n",
    "        )\n",
    "        return output, target\n",
    "\n",
    "    def plot_predictions():\n",
    "        if (i == 0) & (batch_num == 0):\n",
    "            predicted = output[i, -channels:, :, :].cpu().detach()\n",
    "            des_target = target[i, -channels:, :, :].cpu().detach()\n",
    "            fig = plt.figure()\n",
    "            pred = fig.add_subplot(1, 2, 1)\n",
    "            imshow(predicted, title=\"Predicted smoothened %02d\" % n, smoothen=True, obj=pred)\n",
    "            tar = fig.add_subplot(1, 2, 2)\n",
    "            imshow(des_target, title=\"Target %02d\" % n, obj=tar)\n",
    "            plt.show()\n",
    "\n",
    "    model.train()           # initialises training stage/functions\n",
    "    mean_loss = 0\n",
    "    logging.info('Training: Ready to load batches')\n",
    "    for batch_num, batch in enumerate(train_dataloader):\n",
    "        batch_start = time.time()\n",
    "        # logging.info('Batch: %d loaded in %.3f' %(batch_num, batch_time))\n",
    "        mean_batch_loss = 0\n",
    "        Starting_times = random.sample(range(100 - input_frames - (2 * output_frames) - 1), 10)\n",
    "        ImageSeries = batch[\"image\"]\n",
    "        for i, t0 in enumerate(Starting_times):\n",
    "            model.reset_hidden(batch_size=ImageSeries.size()[0], training=True)\n",
    "            exp_lr_scheduler.optimizer.zero_grad()\n",
    "            for n in range(2 * output_frames):\n",
    "                if n == 0:\n",
    "                    output, target = initial_input(channels, training=True)\n",
    "                elif n == output_frames:\n",
    "                    output, target = new_input(output, target, channels, training=True)\n",
    "                else:\n",
    "                    output, target = consequent_propagation(output, target, channels, training=True)\n",
    "                if plot:\n",
    "                    plot_predictions()\n",
    "            loss = F.mse_loss(output, target)\n",
    "            loss.backward()\n",
    "            exp_lr_scheduler.optimizer.step()\n",
    "\n",
    "            mean_batch_loss += loss.item()\n",
    "\n",
    "\n",
    "        analyser.save_loss_batchwise(mean_batch_loss / (i + 1), batch_increment=1)\n",
    "        mean_loss += loss.item()\n",
    "\n",
    "        batch_time = time.time() - batch_start\n",
    "        logging.info(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tTime {:.2f}\".format(epoch, batch_num + 1,\n",
    "                   len(train_dataloader), 100. * (batch_num + 1) / len(train_dataloader), loss.item(), batch_time ) ) \n",
    "        break       \n",
    "\n",
    "\n",
    "    analyser.save_loss(mean_loss / (batch_num + 1), 1)\n",
    "    validation_loss = validate(model, val_dataloader, channels, plot=False)\n",
    "    analyser.save_validation_loss(validation_loss, 1)\n",
    "    logging.info('Validation loss: %.3f ' % validation_loss)\n",
    "\n",
    "\n",
    "\n",
    "def validate(model, val_dataloader, channels, plot=False):\n",
    "    \"\"\"\n",
    "    Validation of network (same protocol as training)\n",
    "    :param val_dataloader: Data to test\n",
    "    :param plot: If to plot predictions\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    def initial_input(channels, training):\n",
    "        Data = ImageSeries[:, (t0 + n) * channels:(t0 + n + input_frames) * channels, :, :].to(device)\n",
    "        output = model(Data, training=training)\n",
    "        target = ImageSeries[:, (t0 + n + input_frames) * channels:(t0 + n + input_frames + 1) * channels, :, :].to(device)\n",
    "        return output, target\n",
    " \n",
    "    def new_input(output, target, channels, training):\n",
    "        output = torch.cat((output, model(\n",
    "            output[:, -input_frames * channels:, :, :].clone(), mode=\"new_input\", training=training)\n",
    "                            ), dim=1)\n",
    "        target = torch.cat(\n",
    "            (target, ImageSeries[:, (t0 + n + input_frames) * channels:(t0 + n + input_frames + 1) * channels, :, :].to(device)), dim=1\n",
    "        )\n",
    "        return output, target\n",
    "\n",
    "    def consequent_propagation(output, target, channels, training):\n",
    "        output = torch.cat((output, model(torch.Tensor([0]), mode=\"internal\", training=training)), dim=1)\n",
    "        target = torch.cat(\n",
    "            (target, ImageSeries[:, (t0 + n + input_frames) * channels:(t0 + n + input_frames + 1) * channels, :, :].to(device)), dim=1\n",
    "        )\n",
    "        return output, target\n",
    "\n",
    "    def plot_predictions():\n",
    "        if (i == 0) & (batch_num == 0):\n",
    "            predicted = output[i, -channels:, :, :].cpu().detach()\n",
    "            des_target = target[i, -channels:, :, :].cpu().detach()\n",
    "            fig = plt.figure()\n",
    "            pred = fig.add_subplot(1, 2, 1)\n",
    "            imshow(predicted, title=\"Predicted smoothened %02d\" % n, smoothen=True, obj=pred)\n",
    "            tar = fig.add_subplot(1, 2, 2)\n",
    "            imshow(des_target, title=\"Target %02d\" % n, obj=tar)\n",
    "            plt.show()\n",
    "\n",
    "    model.eval()\n",
    "    overall_loss = 0\n",
    "    for batch_num, batch in enumerate(val_dataloader):\n",
    "        Starting_times = random.sample(range(100 - input_frames - (2 * output_frames) - 1), 10)\n",
    "        ImageSeries = batch[\"image\"]\n",
    "        batch_loss = 0\n",
    "        for i, t0 in enumerate(Starting_times):\n",
    "            model.reset_hidden(batch_size=ImageSeries.size()[0], training=False)\n",
    "            for n in range(2 * output_frames):\n",
    "                if n == 0:\n",
    "                    output, target = initial_input(channels, training=False)\n",
    "                elif n == output_frames:\n",
    "                    output, target = new_input(output, target, channels, training=False)\n",
    "                else:\n",
    "                    output, target = consequent_propagation(output, target, channels, training=False)\n",
    "                if plot:\n",
    "                    plot_predictions()\n",
    "            batch_loss += F.mse_loss(output, target).item()\n",
    "        break\n",
    "        overall_loss += batch_loss / (i + 1)\n",
    "    val_loss = overall_loss / (batch_num + 1)\n",
    "    return val_loss\n",
    "\n",
    "# get_ipython().system('rm -rf Results/')\n",
    "# get_ipython().system('rm Video_Data/.DS_Store')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Create datasets\n",
      "Optimizer created\n"
     ]
    }
   ],
   "source": [
    "nr_net = 0 \n",
    "\n",
    "version = nr_net + 10\n",
    "input_frames = 5\n",
    "output_frames = 10\n",
    "Type_Network = \"7_kernel_3LSTM\"\n",
    "DataGroup = \"LSTM\"\n",
    "\n",
    "\n",
    "# Little trick to adjust path files for compatibility (I have a backup of the Main.py in case it doesn't work)\n",
    "# stef_path = \"/media/sg6513/DATADRIVE2/MSc/Wavebox/\"\n",
    "# if os.path.isfile(stef_path + \"stefpc.txt\"):\n",
    "#     if not os.path.isdir(stef_path + \"Results\"):\n",
    "#         os.mkdir(stef_path + \"Results\")\n",
    "#     maindir1 = stef_path + \"Results/Simulation_Result_\" + Type_Network + \"_v%03d/\" % version\n",
    "#     maindir2 = stef_path\n",
    "#     version += 200\n",
    "# else:\n",
    "\n",
    "\n",
    "if 'Darwin' in platform.system():\n",
    "    data_dir = './'\n",
    "else:\n",
    "    data_dir = '/disk/scratch/s1680171/wave_propagation/'\n",
    "\n",
    "if not os.path.isdir(\"./Results\"):\n",
    "    os.mkdir(\"./Results\")\n",
    "maindir1 = \"./Results/Simulation_Result_\" + Type_Network + \"_v%03d/\" % version\n",
    "\n",
    "if not os.path.isdir(maindir1):\n",
    "    make_folder_results(maindir1)\n",
    "\n",
    "\n",
    "logging.info('Create datasets')\n",
    "# Data\n",
    "if os.path.isfile(maindir1 + \"All_Data_\" + DataGroup + \"_v%03d.pickle\" % version):\n",
    "    all_data = load(maindir1 + \"All_Data_\" + DataGroup + \"_v%03d\" % version)\n",
    "    train_dataset = all_data[\"Training data\"]\n",
    "    val_dataset = all_data[\"Validation data\"]\n",
    "    test_dataset = all_data[\"Testing data\"]\n",
    "else:\n",
    "    test_dataset, val_dataset, train_dataset = Create_Datasets(\n",
    "         data_dir+\"Video_Data/\", transformVar, test_fraction=0.15, validation_fraction=0.15, check_bad_data=False, channels=channels)\n",
    "    all_data = {\"Training data\": train_dataset, \"Validation data\": val_dataset, \"Testing data\": test_dataset}\n",
    "    save(all_data, maindir1 + \"All_Data_\" + DataGroup + \"_v%03d\" % version)\n",
    "\n",
    "\n",
    "# analyser\n",
    "if os.path.isfile(maindir1 + Type_Network + \"_analyser_v%03d.pickle\" % version):\n",
    "    analyser = load(maindir1 + Type_Network + \"_analyser_v%03d\" % version)\n",
    "else:\n",
    "    analyser = Analyser(maindir1)\n",
    "\n",
    "\n",
    "# Model\n",
    "if os.path.isfile(maindir1 + Type_Network + \"_Project_v%03d.pt\" % version):\n",
    "    model = torch.load(maindir1 + Type_Network + \"_Project_v%03d.pt\" % version)\n",
    "else:\n",
    "    model = Network(device, channels)\n",
    "\n",
    "\n",
    "# Learning Rate scheduler w. optimizer\n",
    "if os.path.isfile(maindir1 + Type_Network + \"_lrScheduler_v%03d.pickle\" % version):\n",
    "    scheduler_dict = load(maindir1 + Type_Network + \"_lrScheduler_v%03d\" % version)\n",
    "    lrschedule = scheduler_dict[\"Type\"]\n",
    "    exp_lr_scheduler = scheduler_dict[\"Scheduler\"]\n",
    "else:\n",
    "    # Optimizer\n",
    "    optimizer_algorithm = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "    # Add learning rate schedulers\n",
    "    # Decay LR by a factor of gamma every step_size epochs\n",
    "    lrschedule = 'plateau'\n",
    "    if lrschedule == 'step':\n",
    "        gamma = 0.5\n",
    "        step_size = 40\n",
    "        exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer_algorithm, step_size=step_size, gamma=gamma)\n",
    "    elif lrschedule == 'plateau':\n",
    "        # Reduce learning rate when a metric has stopped improving\n",
    "        exp_lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer_algorithm, mode='min', factor=0.1, patience=7)\n",
    "\n",
    "logging.info('Optimizer created')\n",
    "\n",
    "\n",
    "# a = train_dataset[0]['image'][0:1,:,:]\n",
    "# imshow(a)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=12)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=True, num_workers=12)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=True, num_workers=12)\n",
    "\n",
    "root_dir = train_dataset.root_dir\n",
    "img_path = train_dataset.All_Imagesets[0]\n",
    "im_list = sorted(listdir(root_dir + img_path[1]))\n",
    "\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info('Version %d' % version)\n",
    "logging.info('Start training')\n",
    "epochs=50\n",
    "for epoch in range(epochs):\n",
    "    epoch_start = time.time()\n",
    "\n",
    "    logging.info('Epoch %d' % epoch)\n",
    "    # for g in exp_lr_scheduler.optimizer.param_groups:\n",
    "    \"\"\"\n",
    "    Here we can access analyser.validation_loss to make decisions\n",
    "    \"\"\"\n",
    "    # Learning rate scheduler\n",
    "    # perform scheduler step if independent from validation loss\n",
    "    if lrschedule == 'step':\n",
    "        exp_lr_scheduler.step()\n",
    "\n",
    "    train_epoch(model, epoch, train_dataloader, val_dataloader, channels, device, plot=False,)\n",
    "    # perform scheduler step if Dependent on validation loss\n",
    "    if lrschedule == 'plateau':\n",
    "        exp_lr_scheduler.step(analyser.validation_loss[-1])\n",
    "    save_network(model, maindir1 + Type_Network + \"_Project_v%03d\" % version, device)\n",
    "    torch.save(model.state_dict(), maindir1 + Type_Network + \"_Project_v%03d.pt\" % version)\n",
    "    save(analyser, maindir1 + Type_Network + \"_analyser_v%03d\" % version)\n",
    "    scheduler_dict = {\"Type\": lrschedule, \"Scheduler\": exp_lr_scheduler.state_dict()}\n",
    "    save(scheduler_dict, maindir1 + Type_Network + \"_lrScheduler_v%03d\" % version)\n",
    "\n",
    "    epoch_time = time.time() - epoch_start \n",
    "    logging.info('Epoch time: %.1f' % epoch_time)\n",
    "\n",
    "# analyser = []\n",
    "# model =[]\n",
    "# exp_lr_scheduler = []\n",
    "# scheduler_dict = []\n",
    "\n",
    "# analyser.plot_loss()\n",
    "# analyser.plot_accuracy()\n",
    "# analyser.plot_loss_batchwise()\n",
    "# analyser.plot_validation_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(exp_lr_scheduler.state_dict(), 'test_scheduler.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "sch = load(maindir1 + Type_Network + \"_lrScheduler_v%03d\" % version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sch['Scheduler'].state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'test.file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hasattr(model, 'module'):\n",
    "    print('has')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.module=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.DataParallel(module=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='model.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Network(device, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_network(model, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = load_network(model, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler_dict['Scheduler'].state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 0.0001\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer_algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scheduler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-168-71f64e5740d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'scheduler' is not defined"
     ]
    }
   ],
   "source": [
    "scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'state': {},\n",
       " 'param_groups': [{'lr': 0.0001,\n",
       "   'betas': (0.9, 0.999),\n",
       "   'eps': 1e-08,\n",
       "   'weight_decay': 0,\n",
       "   'amsgrad': False,\n",
       "   'params': [121583187792,\n",
       "    121583188944,\n",
       "    121583189808,\n",
       "    121583189880,\n",
       "    121207588184,\n",
       "    121207588616,\n",
       "    121207587896,\n",
       "    121207588544,\n",
       "    121207588904,\n",
       "    121207588832,\n",
       "    121207586960,\n",
       "    121207589480,\n",
       "    121207590776,\n",
       "    121207589840,\n",
       "    121207587968,\n",
       "    121207589624,\n",
       "    121207589264,\n",
       "    121207739016,\n",
       "    121207740960,\n",
       "    121207741680,\n",
       "    121207739880,\n",
       "    121207738584,\n",
       "    121207739952,\n",
       "    121207739232,\n",
       "    121207740672,\n",
       "    121207739088,\n",
       "    121207853200,\n",
       "    121207853344,\n",
       "    121207853704,\n",
       "    121207853776,\n",
       "    121207853920,\n",
       "    121207854064,\n",
       "    121207854496,\n",
       "    121207854568,\n",
       "    121207854712,\n",
       "    121207855000,\n",
       "    121207855144,\n",
       "    121207855216,\n",
       "    121207855576,\n",
       "    121207855936,\n",
       "    121207856080,\n",
       "    121207856152,\n",
       "    121207856296,\n",
       "    121207856368,\n",
       "    121207856584,\n",
       "    121207856656]}]}"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_lr_scheduler.optimizer.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct = scheduler_dict['Scheduler'].state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(dct, 'scheduler.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'factor': 0.1,\n",
       " 'min_lrs': [0],\n",
       " 'patience': 7,\n",
       " 'verbose': False,\n",
       " 'cooldown': 0,\n",
       " 'cooldown_counter': 0,\n",
       " 'mode': 'min',\n",
       " 'threshold': 0.0001,\n",
       " 'threshold_mode': 'rel',\n",
       " 'best': 0.0,\n",
       " 'num_bad_epochs': 1,\n",
       " 'mode_worse': inf,\n",
       " 'eps': 1e-08,\n",
       " 'last_epoch': 1}"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_algorithm = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer_algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'factor': 0.1,\n",
       " 'min_lrs': [0],\n",
       " 'patience': 10,\n",
       " 'verbose': False,\n",
       " 'cooldown': 0,\n",
       " 'cooldown_counter': 0,\n",
       " 'mode': 'min',\n",
       " 'threshold': 0.0001,\n",
       " 'threshold_mode': 'rel',\n",
       " 'best': inf,\n",
       " 'num_bad_epochs': 0,\n",
       " 'mode_worse': inf,\n",
       " 'eps': 1e-08,\n",
       " 'last_epoch': -1}"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_scheduler.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler.load_state_dict(dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'state': {},\n",
       " 'param_groups': [{'lr': 0.0001,\n",
       "   'betas': (0.9, 0.999),\n",
       "   'eps': 1e-08,\n",
       "   'weight_decay': 0,\n",
       "   'amsgrad': False,\n",
       "   'params': [121583187792,\n",
       "    121583188944,\n",
       "    121583189808,\n",
       "    121583189880,\n",
       "    121207588184,\n",
       "    121207588616,\n",
       "    121207587896,\n",
       "    121207588544,\n",
       "    121207588904,\n",
       "    121207588832,\n",
       "    121207586960,\n",
       "    121207589480,\n",
       "    121207590776,\n",
       "    121207589840,\n",
       "    121207587968,\n",
       "    121207589624,\n",
       "    121207589264,\n",
       "    121207739016,\n",
       "    121207740960,\n",
       "    121207741680,\n",
       "    121207739880,\n",
       "    121207738584,\n",
       "    121207739952,\n",
       "    121207739232,\n",
       "    121207740672,\n",
       "    121207739088,\n",
       "    121207853200,\n",
       "    121207853344,\n",
       "    121207853704,\n",
       "    121207853776,\n",
       "    121207853920,\n",
       "    121207854064,\n",
       "    121207854496,\n",
       "    121207854568,\n",
       "    121207854712,\n",
       "    121207855000,\n",
       "    121207855144,\n",
       "    121207855216,\n",
       "    121207855576,\n",
       "    121207855936,\n",
       "    121207856080,\n",
       "    121207856152,\n",
       "    121207856296,\n",
       "    121207856368,\n",
       "    121207856584,\n",
       "    121207856656]}]}"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_scheduler.optimizer.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
