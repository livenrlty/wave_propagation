{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf Results/7_kernel_3LSTM_debug*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading datasets\n",
      "Loading analyser\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import logging\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import platform\n",
    "import time\n",
    "from utils.Network import Network\n",
    "from utils.Analyser import Analyser\n",
    "from utils.io import save_network, load_network, save, load, make_folder_results\n",
    "from utils.WaveDataset import create_datasets\n",
    "from utils.training import train_epoch, validate, test\n",
    "\n",
    "logging.basicConfig(format='%(message)s',level=logging.INFO)\n",
    "channels=1\n",
    "num_workers=4\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "transformVar = {\"Test\": transforms.Compose([\n",
    "    transforms.Resize(128),    #Already 184 x 184\n",
    "    transforms.CenterCrop(128),\n",
    "    transforms.ToTensor(),\n",
    "]),\n",
    "    \"Train\": transforms.Compose([\n",
    "    transforms.Resize(128),  # Already 184 x 184\n",
    "    transforms.CenterCrop(128),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    ])\n",
    "}\n",
    "\n",
    "nr_net = 0 \n",
    "\n",
    "version = nr_net + 10\n",
    "num_input_frames = 5\n",
    "num_output_frames = 20\n",
    "reinsert_frequency = 10\n",
    "network_type = \"7_kernel_3LSTM\"\n",
    "\n",
    "if 'Darwin' in platform.system():\n",
    "    data_dir = './'\n",
    "else:\n",
    "    data_dir = '/disk/scratch/s1680171/wave_propagation/'\n",
    "\n",
    "if not os.path.isdir(\"./Results\"):\n",
    "    os.mkdir(\"./Results\")\n",
    "results_dir = \"./Results/\" + network_type + \"_v%03d/\" % version\n",
    "\n",
    "if not os.path.isdir(results_dir):\n",
    "    make_folder_results(results_dir)\n",
    "\n",
    "# Data\n",
    "filename_data = results_dir + \"all_data.pickle\"\n",
    "if os.path.isfile(filename_data):\n",
    "    logging.info('Loading datasets')\n",
    "    all_data = load(filename_data)\n",
    "    train_dataset = all_data[\"Training data\"]\n",
    "    val_dataset = all_data[\"Validation data\"]\n",
    "    test_dataset = all_data[\"Testing data\"]\n",
    "else:\n",
    "    logging.info('Creating new datasets')\n",
    "    test_dataset, val_dataset, train_dataset = create_datasets(\n",
    "         data_dir+\"Video_Data/\", transformVar, test_fraction=0.15, validation_fraction=0.15, check_bad_data=False, channels=channels)\n",
    "    all_data = {\"Training data\": train_dataset, \"Validation data\": val_dataset, \"Testing data\": test_dataset}\n",
    "    save(all_data, filename_data)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=num_workers)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=True, num_workers=num_workers)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "\n",
    "# analyser\n",
    "filename_analyser = results_dir + \"analyser.pickle\" \n",
    "if os.path.isfile(filename_analyser):\n",
    "    logging.info('Loading analyser')\n",
    "    analyser = load(filename_analyser)\n",
    "else:\n",
    "    logging.info('Creating analyser')\n",
    "    analyser = Analyser(results_dir)\n",
    "\n",
    "# Model\n",
    "filename_model = results_dir + \"model.pt\"\n",
    "if os.path.isfile(filename_model):\n",
    "    model = Network(device, channels)\n",
    "    model = load_network(model, device, filename_model)\n",
    "else:\n",
    "    model = Network(device, channels)\n",
    "\n",
    "# Learning Rate scheduler w. optimizer\n",
    "# Optimizer\n",
    "optimizer_algorithm = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "# Add learning rate schedulers\n",
    "# Decay LR by a factor of gamma every step_size epochs\n",
    "scheduler_type = 'plateau'\n",
    "if scheduler_type == 'step':\n",
    "    gamma = 0.5\n",
    "    step_size = 40\n",
    "    lr_scheduler = optim.lr_scheduler.StepLR(optimizer_algorithm, step_size=step_size, gamma=gamma)\n",
    "elif scheduler_type == 'plateau':\n",
    "    # Reduce learning rate when a metric has stopped improving\n",
    "    lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer_algorithm, mode='min', factor=0.1, patience=7)\n",
    "\n",
    "filename_metadata = results_dir + \"metadata.pickle\" \n",
    "meta_data_dict = {  \"optimizer\": optimizer_algorithm.state_dict(),\n",
    "                    \"scheduler_type\": scheduler_type, \n",
    "                    \"scheduler\": lr_scheduler.state_dict()}\n",
    "save(meta_data_dict, filename_metadata)\n",
    "\n",
    "model = model.to(device) \n",
    "\n",
    "starting_point=10\n",
    "from utils.Scorekeeper import Scorekeeper\n",
    "score_keeper=Scorekeeper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Traceback (most recent call last):\n  File \"/Users/stathis/anaconda3/envs/thesis/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py\", line 99, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/Users/stathis/anaconda3/envs/thesis/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py\", line 99, in <listcomp>\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/Users/stathis/Code/thesis/wave_propagation/utils/WaveDataset.py\", line 36, in __getitem__\n    im_list = sorted(listdir(self.root_dir + img_path))\nFileNotFoundError: [Errno 2] No such file or directory: '/disk/scratch/s1680171/wave_propagation/Video_Data/Size-19.41_Centrex0079,y0076'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-1d53a4745d90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m test(model, test_dataloader, starting_point, num_input_frames, num_output_frames, channels, device, \n\u001b[0;32m----> 2\u001b[0;31m      score_keeper, results_dir, plot=True, debug=True)\n\u001b[0m",
      "\u001b[0;32m~/Code/thesis/wave_propagation/utils/training.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(model, test_dataloader, starting_point, num_input_frames, num_output_frames, channels, device, score_keeper, results_dir, plot, debug)\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0mbatch_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    580\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreorder_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__next__\u001b[0m  \u001b[0;31m# Python 2 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_next_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    606\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"KeyError:\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Traceback (most recent call last):\n  File \"/Users/stathis/anaconda3/envs/thesis/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py\", line 99, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/Users/stathis/anaconda3/envs/thesis/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py\", line 99, in <listcomp>\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/Users/stathis/Code/thesis/wave_propagation/utils/WaveDataset.py\", line 36, in __getitem__\n    im_list = sorted(listdir(self.root_dir + img_path))\nFileNotFoundError: [Errno 2] No such file or directory: '/disk/scratch/s1680171/wave_propagation/Video_Data/Size-19.41_Centrex0079,y0076'\n"
     ]
    }
   ],
   "source": [
    "test(model, test_dataloader, starting_point, num_input_frames, num_output_frames, channels, device, \n",
    "     score_keeper, results_dir, plot=True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
